{
  "path": "Makefile",
  "content": ".PHONY: install install-linux install-macos install-windows\n\ninstall: install-linux install-macos install-windows\n\ninstall-linux:\n    python3 -m pip install --user .\n\ninstall-macos:\n    python3 -m pip install --user .\n\ninstall-windows:\n    python -m pip install --user ."
}
{
  "path": "README.md",
  "content": "# Content Processor\n\nContentProcessor is a Python package that scans a directory and processes its contents, generating a structured JSON output with file metadata and content. It provides a convenient way to extract and organize information from a directory hierarchy.\n\n## Features\n\n- Traverses a directory and its subdirectories to process files\n- Excludes specified directories and files from processing\n- Generates a JSON output with file metadata (name, path, size, creation time, modification time, type) and content\n- Supports optional compression of file content using gzip\n- Allows pagination of the output JSON for handling large datasets\n- Provides a command-line interface for easy execution\n\n## Installation\n\nTo install ContentProcessor, follow these steps:\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/your-username/ContentProcessor.git\n```\n\n2. Navigate to the project directory:\n```bash\ncd content_processor\n```\n\n3. Install the package using the Makefile:\n- For Linux: `make install-linux`\n- For macOS: `make install-macos`\n- For Windows: `make install-windows`\n\n## Usage\n\nTo use ContentProcessor, run the following command:\n\n```bash \ncontent_processor <directory> [--compress] [--page-size <size>] [--exclude-dir <dir>] [--exclude-file <file>]\n```\n\n- `<directory>`: The directory to scan and process (required)\n- `--compress`: Compress the file content (optional)\n- `--page-size <size>`: Number of files per page in the output JSON (default: 100)\n- `--exclude-dir <dir>`: Additional directory to exclude (can be specified multiple times)\n- `--exclude-file <file>`: Additional file to exclude (can be specified multiple times)\n\nExample:\n\n```bash \ncontent_processor /path/to/directory --compress --page-size 50 --exclude-dir node_modules --exclude-file .env\n```\n\n## Next Improvements\n\nHere are some potential improvements for ContentProcessor:\n\n1. **Parallel Processing**: Implement parallel processing to speed up the scanning and processing of large directories. This can be achieved using Python's `multiprocessing` module or libraries like `concurrent.futures`. âœ…\n\n2. **Recursive Exclusion**: Allow recursive exclusion of directories and files using wildcard patterns or regular expressions. This would provide more flexibility in specifying exclusion rules.\n\n3. **Metadata Extraction**: Enhance the metadata extraction capabilities by supporting additional file types and extracting more relevant information based on the file type (e.g., image dimensions, video duration).\n\n4. **Output Formats**: Support additional output formats besides JSON, such as CSV or XML, to cater to different use cases and interoperability requirements.\n\n5. **Incremental Processing**: Implement incremental processing functionality, where the package can detect and process only the changes since the last run, instead of processing the entire directory every time.\n\n6. **Error Handling**: Improve error handling and logging mechanisms to provide more informative error messages and facilitate debugging.\n\n7. **Configuration File**: Allow users to specify configuration options (e.g., excluded directories and files) through a configuration file instead of command-line arguments, making it easier to manage and reuse settings.\n\n8. **Integration with Other Systems**: Explore integration possibilities with other systems or platforms, such as databases or cloud storage services, to directly store or process the extracted data.\n\n## Contributing\n\nContributions to ContentProcessor are welcome! If you encounter any issues or have suggestions for improvements, please open an issue or submit a pull request on the [GitHub repository](https://github.com/your-username/ContentProcessor).\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE)."
}
{
  "path": "setup.py",
  "content": "from setuptools import setup, find_packages\n\nsetup(\n    name='content_processor',\n    version='1.0.0',\n    packages=find_packages(),\n    entry_points={\n        'console_scripts': [\n            'content_processor=content_processor.cli:run'\n        ]\n    },\n    install_requires=[\n        'pyexiftool',\n    ],\n    classifiers=[\n        'Programming Language :: Python :: 3',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n    ],\n    python_requires='>=3.6',\n)"
}
{
  "path": "content_processor_system.json",
  "content": ""
}
{
  "path": "content_processor/content_processor.py",
  "content": "import os\nimport gzip\nimport json\nimport fnmatch\nimport mimetypes\nimport exiftool\nfrom datetime import datetime\nimport multiprocessing\nimport argparse\n\n# Define the end-of-file indicator\nEOF_INDICATOR = \"\\\\n---EOF---\\\\n\"\n\n# Default exclusion patterns\nEXCLUDE_DIRS_DEFAULT = {\n    \"node_modules\", \"build\", \".git\", \".svn\", \"dist\",\n    \"__pycache__\", \".pytest_cache\", \"venv\", \".vscode\", \"vendor\",\n    \".idea\", \"bin\", \"obj\", \".DS_Store\", \"tmp\"\n}\n\nEXCLUDE_FILES_DEFAULT = {\n    \".DS_Store\", \".env\", \".env.local\", \".env.docker\", \".env.docker.testing\", \n    \".gitignore\", \".gitkeep\", \".gitattributes\", \".gitmodules\", \"package-lock.json\", \n    \"package.json\", \"vendor\", \"composer.lock\", \"requirements.txt\", \"yarn.lock\", \n    \"yarn-error.log\", \"composer-lock.json\", \"yarn-debug.log\", \"access_log\", \n    \"error_log\"\n}\n\ndef process_file_content(start_directory, file_path, compress=False, exiftool_path=None):\n    try:\n        with open(file_path, 'r', encoding='utf8') as file:\n            content = file.read()\n            if compress:\n                content = gzip.compress(content.encode('utf-8'))\n                content = content.hex()  # Convert compressed content to hexadecimal string\n\n            # Determine the file type using mimetypes\n            mime_type, _ = mimetypes.guess_type(file_path)\n\n            # Extract metadata based on the file type\n            metadata = {}\n            if mime_type:\n                metadata['type'] = mime_type\n\n                # Extract additional metadata using exiftool\n                with exiftool.ExifTool() as et:\n                    metadata_raw = et.execute(b\"-j\", file_path.encode())\n                    metadata.update(json.loads(metadata_raw)[0])\n\n            file_metadata = {\n                \"name\": os.path.basename(file_path),\n                \"path\": os.path.relpath(file_path, start_directory),\n                \"size\": os.path.getsize(file_path),\n                \"created_at\": datetime.fromtimestamp(os.path.getctime(file_path)).isoformat(),\n                \"modified_at\": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),\n                \"metadata\": metadata,\n                \"content\": content\n            }\n            return file_metadata\n    except Exception as e:\n        print(f\"Error processing file {file_path}: {e}\")\n        return None\n\ndef is_excluded(path, exclude_patterns):\n    \"\"\"Checks if the path matches any of the exclude patterns.\"\"\"\n    return any(fnmatch.fnmatch(path, pattern) for pattern in exclude_patterns)\n\ndef process_directory(start_directory, file_list, compress, excluded_dirs, excluded_files, parallel, exiftool_path):\n    if parallel:\n        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n            for root, dirs, files in os.walk(start_directory):\n                # Modify dirs in-place to skip excluded directories\n                dirs[:] = [d for d in dirs if not is_excluded(d, excluded_dirs)]\n                \n                file_paths = [os.path.join(root, filename) for filename in files if not is_excluded(filename, excluded_files)]\n                results = pool.starmap(process_file_content, [(start_directory, file_path, compress, exiftool_path) for file_path in file_paths])\n                \n                file_list.extend(filter(None, results))\n    else:\n        for root, dirs, files in os.walk(start_directory):\n            # Modify dirs in-place to skip excluded directories\n            dirs[:] = [d for d in dirs if not is_excluded(d, excluded_dirs)]\n            \n            for filename in files:\n                file_path = os.path.join(root, filename)\n                if not is_excluded(filename, excluded_files):\n                    file_metadata = process_file_content(start_directory, file_path, compress, exiftool_path)\n                    if file_metadata:\n                        file_list.append(file_metadata)\n\ndef paginate_file_list(file_list, page_size):\n    total_pages = (len(file_list) + page_size - 1) // page_size\n    json_pages = []\n    for page_number in range(total_pages):\n        start_index = page_number * page_size\n        end_index = min(start_index + page_size, len(file_list))\n        page_files = file_list[start_index:end_index]\n\n        # Convert file content back to a string if it's bytes\n        for file in page_files:\n            if isinstance(file['content'], bytes):\n                file['content'] = file['content']\n\n        json_page = {\n            \"page\": page_number + 1,\n            \"total_pages\": total_pages,\n            \"files\": page_files\n        }\n        json_pages.append(json_page)\n    return {\"pages\": json_pages}\n\ndef main(start_directory, compress=False, parallel=False, page_size=100, exclude_patterns=None, exiftool_path='exiftool'):\n    # Check if start directory exists\n    if not os.path.isdir(start_directory):\n        print(f\"The specified start directory '{start_directory}' does not exist or is not a directory.\")\n        return\n\n    exclude_patterns = exclude_patterns or []\n    excluded_dirs = EXCLUDE_DIRS_DEFAULT.union(exclude_patterns)\n    excluded_files = EXCLUDE_FILES_DEFAULT.union(exclude_patterns)\n\n    # Create a list to store file metadata\n    file_list = []\n    \n    # Process directory\n    process_directory(start_directory, file_list, compress, excluded_dirs, excluded_files, parallel, exiftool_path)\n\n    # Paginate the file list\n    json_data = paginate_file_list(file_list, page_size)\n    \n    # Output the final JSON data\n    print(json.dumps(json_data, indent=2))"
}
{
  "path": "content_processor/__init__.py",
  "content": ""
}
{
  "path": "content_processor/cli.py",
  "content": "import argparse\nfrom .content_processor import main\n\ndef run():\n    parser = argparse.ArgumentParser(description='Scan a directory and process its contents.')\n    parser.add_argument('directory', type=str, help='Directory to scan')\n    parser.add_argument('--compress', action='store_true', help='Compress file content')\n    parser.add_argument('--parallel', action='store_true', help='Run processing in parallel')\n    parser.add_argument('--page-size', type=int, default=100, help='Number of files per page')\n    parser.add_argument('--exclude', action='append', help='Exclude pattern (can be specified multiple times)')\n    args = parser.parse_args()\n    main(args.directory, args.compress, args.parallel, args.page_size, args.exclude)\n\nif __name__ == \"__main__\":\n    run()"
}
{
  "path": "content_processor.egg-info/PKG-INFO",
  "content": "Metadata-Version: 2.1\nName: content_processor\nVersion: 1.0.0\nClassifier: Programming Language :: Python :: 3\nClassifier: License :: OSI Approved :: MIT License\nClassifier: Operating System :: OS Independent\nRequires-Python: >=3.6\nRequires-Dist: pyexiftool\n"
}
{
  "path": "content_processor.egg-info/SOURCES.txt",
  "content": "README.md\nsetup.py\ncontent_processor/__init__.py\ncontent_processor/cli.py\ncontent_processor/content_processor.py\ncontent_processor.egg-info/PKG-INFO\ncontent_processor.egg-info/SOURCES.txt\ncontent_processor.egg-info/dependency_links.txt\ncontent_processor.egg-info/entry_points.txt\ncontent_processor.egg-info/requires.txt\ncontent_processor.egg-info/top_level.txt"
}
{
  "path": "content_processor.egg-info/entry_points.txt",
  "content": "[console_scripts]\ncontent_processor = content_processor.cli:run\n"
}
{
  "path": "content_processor.egg-info/requires.txt",
  "content": "pyexiftool\n"
}
{
  "path": "content_processor.egg-info/top_level.txt",
  "content": "content_processor\n"
}
{
  "path": "content_processor.egg-info/dependency_links.txt",
  "content": "\n"
}
